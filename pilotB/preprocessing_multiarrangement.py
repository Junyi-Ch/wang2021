# This code is generated by Claude, adapting from the original data analyses code the authors provided, which was in Matlab.
"""
Preprocessing for Multiarrangement Data
Implements the weighted averaging algorithm from Kriegeskorte & Mur (2012)
"""
import pandas as pd
import numpy as np
import json
import os
import glob
from scipy.spatial.distance import squareform
from tqdm import tqdm

N_WORDS = 90  # Total number of words in full set
JSON_COLUMN_NAME = 'dissimilarity_vector'

def load_and_combine_multiarrangement_trials(data_folder, equal_weights=True):
    """
    Loads all trials (full + subsets) and combines them using weighted averaging.
    
    Parameters:
    -----------
    data_folder : str
        Folder containing participant CSV files
    equal_weights : bool
        If True, use equal weights for all trials (simple averaging)
        If False, use squared distances as weights (as in paper)
    
    Returns:
    --------
    all_rdms : ndarray
        Array of shape (n_participants, N_WORDS, N_WORDS) with averaged RDMs
    participant_ids : list
        List of participant IDs
    """
    
    # Load all CSV files
    csv_files = glob.glob(os.path.join(data_folder, '*.csv'))
    if not csv_files:
        raise FileNotFoundError(f"No .csv files found in folder: {data_folder}")
        
    print(f"Found {len(csv_files)} CSV files. Processing...")
    
    all_rdms = []
    participant_ids = []
    
    for csv_file in tqdm(csv_files, desc="Processing participants"):
        df = pd.read_csv(csv_file)
        
        # Filter for rows with dissimilarity data
        data_rows = df[df[JSON_COLUMN_NAME].notna()].copy()
        
        if len(data_rows) == 0:
            print(f"Warning: No data in {os.path.basename(csv_file)}")
            continue
        
        # Get participant ID
        participant_id = data_rows['participant_number'].iloc[0]
        
        try:
            # Process this participant's trials
            rdm = combine_trials_for_participant(data_rows, equal_weights)
            
            all_rdms.append(rdm)
            participant_ids.append(participant_id)
            
        except Exception as e:
            print(f"Error processing {participant_id}: {e}")
            continue
    
    if len(all_rdms) == 0:
        raise ValueError("No participants were successfully processed")
    
    print(f"\nSuccessfully processed {len(all_rdms)} participants")
    return np.array(all_rdms), participant_ids


def combine_trials_for_participant(data_rows, equal_weights=True):
    """
    Combines multiple trials for a single participant using weighted averaging.
    
    Parameters:
    -----------
    data_rows : DataFrame
        Rows containing trial data for one participant
    equal_weights : bool
        Whether to use equal weights (True) or distance-based weights (False)
    
    Returns:
    --------
    final_rdm : ndarray
        89x89 dissimilarity matrix averaged across all trials
    """
    
    # Step 1: Get the master word list from the full trial (89 words)
    full_trial = data_rows[data_rows['n_words'] == N_WORDS]
    
    if len(full_trial) == 0:
        raise ValueError(f"No full trial ({N_WORDS} words) found for this participant")
    
    if len(full_trial) > 1:
        print(f"  Warning: Multiple full trials found, using first one")
        full_trial = full_trial.iloc[:1]
    
    # Extract master word list (ordered)
    placements_json = full_trial['placements'].iloc[0]
    placements = json.loads(placements_json)
    master_word_list = [p['word'] for p in placements]
    
    if len(master_word_list) != N_WORDS:
        raise ValueError(f"Master word list has {len(master_word_list)} words, expected {N_WORDS}")
    
    # Create word to index mapping
    word_to_idx = {word: i for i, word in enumerate(master_word_list)}
    
    # Initialize accumulation matrices
    sum_matrix = np.zeros((N_WORDS, N_WORDS))
    count_matrix = np.zeros((N_WORDS, N_WORDS))
    
    # Step 2: Process each trial
    for idx, row in data_rows.iterrows():
        n_words = int(row['n_words'])
        
        # Get word list for this trial
        placements = json.loads(row['placements'])
        trial_words = [p['word'] for p in placements]
        
        # Get dissimilarity vector for this trial
        dissim_vec = json.loads(row[JSON_COLUMN_NAME])
        
        # Convert vector to square matrix
        expected_len = (n_words * (n_words - 1)) // 2
        if len(dissim_vec) != expected_len:
            print(f"  Warning: Trial with {n_words} words has vector length {len(dissim_vec)}, expected {expected_len}")
            continue
        
        trial_matrix = squareform(dissim_vec)
        
        # Determine weight for this trial
        if equal_weights:
            weight = 1.0
        else:
            # Weight by squared distances (evidence weight from paper)
            weight = np.mean(dissim_vec) ** 2
        
        # Map trial distances to full matrix
        for i in range(n_words):
            for j in range(i+1, n_words):
                word_i = trial_words[i]
                word_j = trial_words[j]
                
                # Get indices in master word list
                try:
                    master_i = word_to_idx[word_i]
                    master_j = word_to_idx[word_j]
                except KeyError as e:
                    print(f"  Warning: Word {e} not in master list")
                    continue
                
                # Add to sum (symmetric)
                distance = trial_matrix[i, j]
                sum_matrix[master_i, master_j] += weight * distance
                sum_matrix[master_j, master_i] += weight * distance
                
                # Increment count (symmetric)
                count_matrix[master_i, master_j] += weight
                count_matrix[master_j, master_i] += weight
    
    # Step 3: Compute weighted average
    # Avoid division by zero
    with np.errstate(divide='ignore', invalid='ignore'):
        final_rdm = sum_matrix / count_matrix
    
    # Set diagonal to 0 (self-dissimilarity)
    np.fill_diagonal(final_rdm, 0)
    
    # Check for any NaN values (pairs never measured)
    n_nan = np.sum(np.isnan(final_rdm))
    if n_nan > 0:
        print(f"  Warning: {n_nan} word pairs were never measured together")
        # Fill NaNs with mean of available dissimilarities
        final_rdm = np.nan_to_num(final_rdm, nan=np.nanmean(final_rdm))
    
    return final_rdm


def save_rdms_to_mat_files(all_rdms, participant_ids, output_folder):
    """
    Optionally save RDMs as .mat files for MATLAB compatibility.
    """
    try:
        import scipy.io as sio
        os.makedirs(output_folder, exist_ok=True)
        
        for rdm, participant_id in zip(all_rdms, participant_ids):
            # Convert to vector form (lower triangle)
            vector_form = squareform(rdm, checks=False)
            
            mat_file_path = os.path.join(output_folder, f'dismx_{participant_id}.mat')
            sio.savemat(mat_file_path, {'estimate_dissimMat_ltv': vector_form})
        
        print(f"Saved {len(all_rdms)} .mat files to {output_folder}")
    except ImportError:
        print("scipy.io not available, skipping .mat file export")


# Example usage
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python preprocessing_multiarrangement.py <data_folder> [output_folder]")
        print("\nExample:")
        print("  python preprocessing_multiarrangement.py ./my_data ./output")
        sys.exit(1)
    
    data_folder = sys.argv[1]
    output_folder = sys.argv[2] if len(sys.argv) > 2 else './preprocessed'
    
    # Load and combine trials
    all_rdms, participant_ids = load_and_combine_multiarrangement_trials(
        data_folder, 
        equal_weights=True
    )
    
    print(f"\nFinal dataset shape: {all_rdms.shape}")
    print(f"Participants: {participant_ids}")
    
    # Save as numpy array
    os.makedirs(output_folder, exist_ok=True)
    np.save(os.path.join(output_folder, 'all_rdms.npy'), all_rdms)
    
    # Save participant info
    participant_df = pd.DataFrame({'participant_id': participant_ids})
    participant_df.to_csv(os.path.join(output_folder, 'participant_info.csv'), index=False)
    
    # Optionally save as .mat files
    save_rdms_to_mat_files(all_rdms, participant_ids, output_folder)
    
    print(f"\nPreprocessing complete! Files saved to {output_folder}")
    print(f"  - all_rdms.npy: shape {all_rdms.shape}")
    print(f"  - participant_info.csv")
